{"cells":[{"metadata":{"_uuid":"3abbc49d2ccbaefc1f47f8111a208f8814b4ee9b","_cell_guid":"635ba0f6-66eb-4743-b83d-9a25277b4968"},"cell_type":"markdown","source":"# Wk 2. Lagmart, Rik's take on Enea's brilliant insight\nSo the competition is over. We had a very brief summary of the winner's approach. Enea Caccia (group 8) scored a loss (MAE) of 2144 on the private leaderboard. Our attempts kept lagging around 13000. The surprising bit was that only 5 teams (out of 11) did better than the 2650 hurdle.  The rest of us scored 12950 or worse.\n\nMy old model simply tried to enhance the simplistic Baseline Jannes Klaas had provided. That was the simplest one layer linear regressor possible. I tried to improve it by making the NN more complicated; more layers, more units and activation functions. My feature engineering was a convoluted attempt to compute distance from today to the nearest holiday.\n\nNo matter how I tried, the model kept giving me predictions that were all the same number for 120000 different inputs. It was time for a radical change, but I did not know what to radically change. Until Enea's debriefing yesterday...\n\n## Enea's humble median\nThe winning entry had not even used a neural net. Enea computed the median from grouped samples:\n* median per Store per Dept per Month (of year) per IsHoliday (boolean field in the data)\n\nHere is [his kernel](https://www.kaggle.com/eneacaccia/week2-challenge-simple-stats). Here is my cleaned up, completed [version](https://www.kaggle.com/rikdedeken/week2-challenge-simple-stats). Both score a Loss = 2144.\n\n\n## My own variation: ensemble of means\nSo precompute median Weekly_Sales heh? Bene. I can do that. I can even precompute a handfull. Throw away every other bit of input? Easy. Than find a suitable weighted average of the precomputed numbers? Hey! That sounds like an optimization job for a neural net....\n\nThat's what I do in this notebook. And it still did not work. **Until**.... I threw away my activation functions and most of the layers.\n\nMy model now looks something like this:\n\n11 input units -> 1 output unit\n\nWithout any activation function.\n\n## Data and features\nI precomputed the following medians:\n* StoreMean (an average of all Sales for a particular store, over the entire rage of dates)\n* DeptMean (an average of all Sales for a particular department ID, across the whole Walmart chain, for all dates)\n* Store_DeptMean (an average of all Sales for this dep in this store, for all dates)\n* MonthMean (an average for all Sales this month of the year, across the Walmart chain)\n* Store_MonthMean (an average for this month, this store, all dates)\n* Dept_MonthMean (an average for this month, this department ID, across the chain, for al dates)\n* Store_Dept_MonthMean (an average for this month, this Dept, in this Store)\n* IsHolidayMean (one average from all samples that have IsHoliday=true)\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a65b09f59aa16561275772149b0ec286a69e7a10","_cell_guid":"9913d1bc-ef96-4d1c-b7df-f61e4ee15555"},"cell_type":"markdown","source":"The input consists of:\n\n1. IsHoliday \n2. Month\n3. Week\n4. MonthMean\n5. Store_MonthMean\n6. Dept_MonthMean\n7. Store_DeptMean\n8. IsHolidayMean\n9. StoreMean\n10. DeptMean\n11. Store_Dept_MonthMean\n\nNone of these were scaled, standardized, normalized or anything. I fed them raw into the NN.\n       ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"21dfe8d6c66bd192897c6d7edc9c8a56aafd081a","_cell_guid":"a5f88fe5-930f-407a-8de6-384b15666950"},"cell_type":"markdown","source":"## my best score so far?\n\n\n\n\nDuring training I validated on 20% of training data. The Loss got as low as:\n```\nLoss by Mean Absolute Error\ntrain:       1720.68063151\nvalidation:  1721.73183077\n```\nAfter submitting these predictions to Kaggle, I scored ```2018.75718```\n\nMy latest submissions:\n```\n.       train/val  -> kaggle\nfeb 21: 1650/1683  -> 2025 (one layer NN: in>out)\nfeb 21: 1656/1661  -> 2043 (two layer NN: in>2>out)\n```","outputs":[],"execution_count":null},{"metadata":{"_uuid":"dca9dffc8dd47aa5fcf90f05a7015b84b95a70d4","_cell_guid":"70d37585-b6cc-4103-842a-5752fd96ffd6"},"cell_type":"markdown","source":"## Possible improvements\n* normalize inputs\n* bigger NN\n* use actual .mean() instead of .median()\n* fewer inputs\n* more inputs\n* train more/longer\n* final training round with 100% training data (who needs validation that late in the race anyway?)\n* reintroduce optimization layers (batchnorm, dropout), tell the model to regularize (still trying to find out where I went wrong in the first place!)\n\n## Possible code improvements\n* Make computations of grouped means easier on the CPU. Use the groupby() method in combination with merge() or join().\n* Prettier graphs 8-)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a50db2beaca5e2ce4361624a3ff805fa39731a86","_cell_guid":"a229a5da-2d43-46c2-8551-f3c5be8a1521","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.layers import Dense, Activation, BatchNormalization, Dropout\nimport keras.models\nfrom keras import regularizers\nfrom keras import optimizers\n\nimport re, datetime, time\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e498185e895a8a46e90b329c011496069517e9b1","_cell_guid":"06a25e3a-14e8-48f9-be71-9f714b1dd2de","trusted":true},"cell_type":"code","source":"print(datetime.datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"195dd6f20e7015db70a57cd79b2d4f937761bfd4","_cell_guid":"6744355d-9408-4072-aa11-4094ca35719d"},"cell_type":"markdown","source":"## load data from CSV and organize","outputs":[],"execution_count":null},{"metadata":{"scrolled":false,"_uuid":"408737d9ac474ad8b5a06e8635c2b58764c0fbde","_cell_guid":"46649bda-0527-4120-b9e4-31eb5e3f0fbe","trusted":true},"cell_type":"code","source":"# load hard work on features from csv files\n! find ../input ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"e6cdd5045707ec32e115e7c361b17fd7aa2b8449","_cell_guid":"da5d69ca-ecf2-464c-940d-3002f1d55296","trusted":true},"cell_type":"code","source":"# @rik: the only reason you indexed Date is to produce a date_plot\n# with blanks in it at all the right places\n\n%xdel train\n%xdel test\n%xdel train_f\n%xdel test_f\n%xdel df_b\n%xdel df_f\n%xdel df\n%xdel pristine\n\n# choose: precomputed features, or bare Walmart data, or both\n\n# precomputed features:\n# train_f = pd.read_csv('../input/bletchley-wk2-featured-walmart/train_features_redux2.csv')\n# test_f = pd.read_csv('../input/bletchley-wk2-featured-walmart/test_features_redux2.csv')\n# df_f = pd.concat([train_f,test_f],axis=0) # Join train and test\n# df_f.drop(['IsHoliday', 'Weekly_Sales'], inplace=True, axis=1)\n\n# bare Walmart:\ntrain = pd.read_csv('../input/course-material-walmart-challenge/train.csv')\ntest = pd.read_csv('../input/course-material-walmart-challenge/test.csv')\n\n# train = pd.read_csv('../input/train.csv')\n# test = pd.read_csv('../input/test.csv')\n\ndf = pd.read_csv('../input/bletchley-wk2-featured-walmart/df_features_redux2.csv')\n\n# df = pd.concat([train,test],axis=0) # Join train and test\n# df_b = pd.concat([train,test],axis=0) # Join train and test\n# df = pd.concat([df_b,df_f],axis=1) \n\nlen_train = len(train)\nlen_test = len(test)\n\npristine = df.copy()\n# pristine['Date'] = pd.to_datetime(df['Date'])\n#### DO NOT SORT concatenated df: this will make the cat irreversible!!!!!!!!!!!!!!!!!!!!!!!\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44c0e19e089fa879e4c014d2278dbed160d79828","_cell_guid":"ff0a1631-b3b1-4da6-8363-f37b59e05339","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf291c70b1939012c6f2f8124ca05b740aebef6","_cell_guid":"7fe45cfe-7eb2-40cc-a376-28d3d43472d3"},"cell_type":"markdown","source":"## new features: categories from date","outputs":[],"execution_count":null},{"metadata":{"_uuid":"ee5a6a9d2e7a3ce0056dfa7b5582f55948539a78","collapsed":true,"_cell_guid":"37e18e30-5860-41b2-8f1e-506e3994c33e","trusted":true},"cell_type":"code","source":"def add_datepart(df, fldname, parts=[], drop=True):\n    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n    the information from the date. This applies changes inplace.\n    \"\"\"\n\n    # also available on Kaggle: import fastai\n    # This is my adaptation\n    # Acknowledgment: Fast.ai ML library https://github.com/fastai/fastai/blob/master/fastai/structured.py\n    # Jeremy Howard c.s.\n\n    if len(parts) == 0:\n        parts = [\n            'Year', \n            'Month', \n            'Week', \n            'Dayofyear',\n            'Dayofweek', \n            'Day', \n            'Is_month_end', \n            'Is_month_start', \n            'Is_quarter_end', \n            'Is_quarter_start', \n            'Is_year_end', \n            'Is_year_start',\n            'Elapsed',\n                ]\n\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n\n    # remove the word \"date\" from the end of the fldname\n    targ_pre = re.sub('[Dd]ate$', '', fldname) \n    for n in parts:\n        target = targ_pre+n\n        if not target in df.columns:\n            df[target] = getattr(fld.dt,n.lower())\n            if n == 'Elapsed':\n                df[targ_pre+'Elapsed'] = fld.astype(np.int64) // 10**9\n\n    if drop: df.drop(fldname, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0f6ce15863857f887f5e1ba1d5fee94ef5dd6e3","collapsed":true,"_cell_guid":"df27858a-960c-4f49-a8bc-e781fc146ea4","trusted":true},"cell_type":"code","source":"# This clever function adds numerical and boolean fields for every(?) conceivable attribute of a Date object as new colums to df\nadd_datepart(df, 'Date', parts=['Year', 'Month', 'Week'], drop=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fef50bae470d279e59633be6acb6c2d4797e649b","_cell_guid":"734d5574-bb78-47d6-8fda-3e8b296ed3fc"},"cell_type":"markdown","source":"## new features: boolean (dummy) categories from categories","outputs":[],"execution_count":null},{"metadata":{"_uuid":"34a8b729071bf4df7b343de5a0dad3a4c4a12417","collapsed":true,"_cell_guid":"44023b5c-a79c-453e-89eb-ea608ef782a2","trusted":true},"cell_type":"code","source":"# create dummy features/columns for categorical data\n\ncatcats = [\n#     \"Year\",\n#     \"Month\",\n#     \"Type\",\n#     \"Store\",\n#     \"Dept\",\n]\n\nfor c in catcats:\n    if c in df.columns:\n        dummies = pd.get_dummies(df[c], prefix=c, prefix_sep='')\n        df = pd.concat([df, dummies], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8ac7811f94fd39d0b81d121e33a0d0aef71df7","_cell_guid":"09e1d269-b94f-44ed-8263-b7e49d2ea255"},"cell_type":"markdown","source":"# new feature: mean weekly sales\n\nper Store, per Dept, per Month, per IsHoliday, we establish the mean Weekly_Sales (overall mean is ca 10445). \nLet's introduce these means into a column of their own. \nThis idea was heavily inspired by Enea's winning entry in the in-class competition at Bletchley Bootcamp, 19 feb 2018.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d602f450e4905854e56bc168515ca487ede3a389","collapsed":true,"_cell_guid":"59df8cb8-5b00-4d2d-8cc3-6a96fa352c64","trusted":true},"cell_type":"code","source":"def add_feature_mean_y(df, feature, y):\n    # create new feature in column of df: fill with mean of y for every category of feature\n    # the new column gets a name: name of given feature + \"Mean\"\n    cname = \"\".join([feature, \"Mean\"])\n    if cname in df.columns: return(False)\n    \n    df[cname] = df[y].mean() # create column, fill with not so random value\n    \n    # iterate over all unique categories in given feature\n    # for combined features (e.g. Store_Dept_Month) this takes a LOT of compute time (say 1 hour)\n    for k in df[feature].unique():\n        # in rows of cat k, to new column , assign of all rows of cat k the mean(y)\n#         mean = train.loc[train[feature] == k, [y]].median().values[0]\n        mean = df.loc[df[feature] == k, [y]].median().values[0]   ## FIXME if shit happens 201802211020\n        df.loc[df[feature] == k, [cname]] = mean\n    return(cname)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b9d8236c0a883e6a75beff68a68d6ad7383548","collapsed":true,"_cell_guid":"938eeb96-eeda-43ca-ac5a-47b33cc55617","trusted":true},"cell_type":"code","source":"def concat_features(df, features):\n    # create new categorical feature named after a concatenation of the individual feature names\n    # fill with concatenated values of those feature\n    # example: features named   Given + Family = Given_Family\n    #          feature values   joan  + jet    = joan_jet\n    # separator is underscore _ for both names and values\n    # any number of features (>1) can be concatenated\n    # df['Store_Dept'] = df[['Store', 'Dept']].astype(str).apply(lambda x: '_'.join(x), axis=1)\n    cname = '_'.join(features)\n    if cname in df.columns: return(False)\n\n    df[cname] = df[features].astype(str).apply(lambda x: '_'.join(x), axis=1)\n    return(cname)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be639076ab27d4a05c0e3caa6b38c883410e8246","_cell_guid":"6d0ef2b2-308f-4957-863c-3e2c0b2c515b"},"cell_type":"markdown","source":"## create new features: combined categories","outputs":[],"execution_count":null},{"metadata":{"_uuid":"c1f68b4bb7f2ad01942a4e1031c45133ca5536d1","_cell_guid":"e7e9ac88-7b70-49e2-ab40-bb703670b029","trusted":true},"cell_type":"code","source":"concat_features(df, ['Store', 'Month'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a70a90ebdae0176b80959c36747708fdcb2cc7ca","_cell_guid":"dbbc9501-0681-49be-b77d-3c97d4d40a0b","trusted":true},"cell_type":"code","source":"concat_features(df, ['Dept', 'Month'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f99a80a96124f1d43e04d3837bd981148349ffb","_cell_guid":"1c4e5211-7dac-4eb8-bc79-129c653f9eb3","trusted":true},"cell_type":"code","source":"concat_features(df, ['Store', 'Dept'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab9e502b0626c73d0e804aba42b19fda4aab94f2","_cell_guid":"cc4b0f9d-7721-43cb-a684-65f86def6379","trusted":true},"cell_type":"code","source":"concat_features(df, ['Store_Dept', 'Month'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab4d61f68ba0386a8cdfdf825a122fb2dc1bac78","_cell_guid":"a09cecd3-af67-428a-9896-dcd58824f4a6","trusted":true},"cell_type":"code","source":"concat_features(df, ['Year', 'Month'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a362197016ff2ad5e466f22c7dad41c62348b96","_cell_guid":"41103a0e-4412-490c-a607-cc76a5de98e3"},"cell_type":"markdown","source":"## create features: mean_y for categories","outputs":[],"execution_count":null},{"metadata":{"_uuid":"cb1db2914570a81fdfccaab34ee3988618ded304","collapsed":true,"_cell_guid":"b1f8df7c-34c2-4feb-b866-981acd3ac1a7","trusted":true},"cell_type":"code","source":"# we make this split in between feature engineering, because we want to extract mean values from the y column\n# and we do not want the zero ys from the test set to drag the mean down\n# train = df.iloc[:len_train]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5d284ba3f9b29bf0d9c74080f2ebd0541a93dd1","_cell_guid":"30343f45-af92-4f7b-8ca9-8e224d7da6ca","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Store', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"950696c0aa8431ee25a6dac9445e4c89617fb46f","_cell_guid":"7c1d4216-9dd1-447e-87ad-66a8d833f5f3","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Dept', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Store_Dept', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c08fca3029b3b1413a0530ddda0baf9700b1f6c","_cell_guid":"a4cb3773-71ee-4ba2-89ff-1377bd2f4cee","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Month', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Store_Month', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Dept_Month', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Store_Dept_Month', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'IsHoliday', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"6580b67d8df996dc90c2c00d5ce5ff83c73e8507","_cell_guid":"844b5427-5370-43c4-b07f-fb242d4e4c05","trusted":true},"cell_type":"code","source":"add_feature_mean_y(df, 'Year_Month', 'Weekly_Sales')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98b222c2322ad021c987e394d0299a4f7495c66","_cell_guid":"53de9ed6-b569-41e2-a21b-e4db3e296831"},"cell_type":"markdown","source":"## nice graphics","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b0071029a653078d21734cf41498c21fe0439c9a","collapsed":true,"_cell_guid":"1eb50987-eb3a-414a-ba30-7340bd87b924","trusted":true},"cell_type":"code","source":"# plots sales and holidays in time for specific Store/Dept\ndef plot_store_dept(plotme):\n    wsm = plotme.Weekly_Sales.median()\n    fig, ax = plt.subplots(figsize=(13,5))\n    ax.set_ylabel(\"Weekly_Sales\")\n    ax.bar(x=plotme.index, height=plotme[\"Weekly_Sales\"], color='g', width=7, label=\"known sales\")\n    ax.axhline(y=wsm, c='g', label=\"Store_DeptMean\")\n    ax.plot_date(x=plotme.index, y=(plotme[\"IsHoliday\"] * wsm), fmt='*m', ms=16)\n    ax.plot_date(x=plotme.index, y=plotme['Dept_MonthMean'], fmt='.k', ms=3)\n    ax.plot_date(x=plotme.index, y=plotme['Store_MonthMean'], fmt='.y')\n    ax.plot_date(x=plotme.index, y=plotme['Store_Dept_MonthMean'], fmt='+c')\n    ax.legend()\n    plt.title(f\"Store: {store}\\nDept: {dept}\")\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2131b1840089e5f69b42e6c0fd44dc06586c963f","collapsed":true,"_cell_guid":"72324a94-7b67-48a2-90ee-baaa4b2ef512","trusted":true},"cell_type":"code","source":"store=1\ndept=1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"78fddcab4edab136b08f19618c92a7f2d4abb35f","_cell_guid":"b3374b5f-f936-4650-83d8-d47cb64955f7","trusted":true},"cell_type":"code","source":"store += 1\nwhere = (df[\"Store\"] == store) & (df[\"Dept\"] == dept) \nselect = ['Date', 'Weekly_Sales', 'IsHoliday', 'Store_MonthMean', 'Dept_MonthMean', 'Store_Dept_MonthMean']\nplotme = df.loc[where, select]\nplotme.set_index('Date', inplace=True)\nplot_store_dept(plotme)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"2a68eacb355c4a72a3119a01738f466c833348ed","_cell_guid":"5efef214-dc4b-49f0-9378-3b29a504ee5c"},"cell_type":"markdown","source":"## last minute cleanup\nmostly categorical noise and features that we do not want our model exposed to, also fill Nulls (Nans) with zeroes, but most of all save save save!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1b6b9893fbb85f019c7423ff103af85093b68498","collapsed":true,"_cell_guid":"1dd157e7-7394-4916-afc6-d5384c54977c","trusted":true},"cell_type":"code","source":"df.fillna(0, inplace=True) # fill NAs with zeroes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133d16b5f3abdf3c47217205bef74d519bc4849b","collapsed":true,"_cell_guid":"4c54e8fb-a9f9-42d5-b674-52a0a01f1e3b","trusted":true},"cell_type":"code","source":"# df.to_csv('df_features_redux2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63aaec69263b778f462a055bf71d64f696c02860","collapsed":true,"_cell_guid":"8f72d8a8-4486-438b-a91f-e975de193be1","trusted":true},"cell_type":"code","source":"drop_these = [\n    'Date',\n    'Store',\n    'Dept',\n    'Type',\n    'IsHoliday',\n\n    'CPI',\n    'Fuel_Price',\n    'MarkDown1',\n    'MarkDown2',\n    'MarkDown3',\n    'MarkDown4',\n    'MarkDown5',\n    'Size',\n    'Temperature',\n    'Unemployment',\n\n    'Week',\n    'Month',\n    'Year',\n    'Year_Month',\n    'Store_Month',\n    'Dept_Month',\n    'Store_Dept',\n    'Store_Dept_Month',\n\n#     'MonthMean',\n#     'Store_MonthMean',\n#     'Store_Dept_MonthMean',\n#    'IsHolidayMean',\n#     'StoreMean',\n#     'DeptMean',\n        ]\n\nfor dropje in drop_these:\n    if dropje in df.columns:\n        df.drop(dropje, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"cd78d92d6885fc8b44b0a8fd4020aa456fcd9ac7","_cell_guid":"b1fc2009-1d3a-4704-b6da-00424182aac9","trusted":true},"cell_type":"code","source":"# final sanity check\ndf[:12]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4daa2af6748c3582ba8c2ffddd81a4930632478d","_cell_guid":"139537c2-034d-4382-86c3-b992fc12e395","trusted":true},"cell_type":"code","source":"for c in df.columns.sort_values():\n    print(f\"\\'{c}\\',\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e80682caa0737cc0733a7a4f58bdaa29ccc8b476","_cell_guid":"123d8834-a393-44d1-a2c4-4ba42c6d6ed6"},"cell_type":"markdown","source":"# split df into train + test and (maybe) val","outputs":[],"execution_count":null},{"metadata":{"_uuid":"cb1db2914570a81fdfccaab34ee3988618ded304","collapsed":true,"_cell_guid":"b1f8df7c-34c2-4feb-b866-981acd3ac1a7","trusted":true},"cell_type":"code","source":"train = df.iloc[:len_train]\nX_labels = train.drop('Weekly_Sales',axis=1).columns.astype(str)\nX = train.drop('Weekly_Sales',axis=1).values\ny = train['Weekly_Sales'].values\n\ntest = df.iloc[len_train:]\ntest = test.drop('Weekly_Sales',axis=1) # We should remove the nonsense values from test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29e9bea1bc40aa340c3ec84a965c1ddba905e299","collapsed":true,"_cell_guid":"8ffc9d36-ecdd-4255-9488-ae5b32eb03ae","trusted":true},"cell_type":"code","source":"# save hard work on features in new csv files\n# train.to_csv('train_features_redux2.csv',index=False)\n# test.to_csv('test_features_redux2.csv',index=False)\n# ! ls -l *csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"151c97d591ccaf932a6433a27dd629fb4966c6e1","_cell_guid":"c5833399-1f27-4742-98b5-14e81bceb88e","trusted":true},"cell_type":"code","source":"# Split training set into sets for training and validation\n# or, alternatively: order model.fit() to take its own splits\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\nX=X_train\ny=y_train\nprint(X.shape)\nprint(X_val.shape)\nm_val,n_val = X_val.shape\nm,n = X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01012332a7257684c42313aec2d71f0830e5011d","_cell_guid":"dd1bf05a-1eff-4ac8-87eb-8dc284dba25d"},"cell_type":"markdown","source":"## Tweak your hyper parameters here","outputs":[],"execution_count":null},{"metadata":{"_uuid":"17f0be861b367944be90baa3afe2663752e06dd9","collapsed":true,"_cell_guid":"4af64497-aca2-4234-b40f-3df13b979daa","trusted":true},"cell_type":"code","source":"lamb = .01   # regularization rate\ndrop = 0.10  # dropout rate","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"969608483f32f3fbd3c21731c0e0b9a2e0babd35","_cell_guid":"0c6c1b13-2619-42a4-aa5a-bfdc835d69e4","trusted":true},"cell_type":"code","source":"#, kernel_regularizer = regularizers.l2(lamb)\n# model.add(BatchNormalization())\n# model.add(Activation('tanh'))\n# model.add(Dropout(rate=drop))\n\n%xdel model\nmodel = keras.models.Sequential()\n\nmodel.add(Dense(units=n, input_dim=n))\nmodel.add(Dense(units=n))\nmodel.add(Dense(units=3))\n# model.add(BatchNormalization())\n\nmodel.add(Dense(units=1))\n\nmodel.compile(optimizer=optimizers.adam(), loss='mae')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"5b69dd7ec45ae919c9fd347c34df63e5b9d47ed1","_cell_guid":"03a6e800-359b-4d08-a016-a48e5f1346f1","trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de797f67c50f2d77fa4513a5f6f6326076efe247","_cell_guid":"df8b8301-d60c-40a0-bec6-c24be5f53e46"},"cell_type":"markdown","source":"## alpha, batchsize, epochs","outputs":[],"execution_count":null},{"metadata":{"_uuid":"eeb75625a4b039b6db5bafdeb14db7ab79ce7635","collapsed":true,"_cell_guid":"b037f095-4de1-44f3-8d54-4aca0c645395","trusted":true},"cell_type":"code","source":"# these hyper parameters can be tweaked after compiling the model\n# this is useful for retraining an existing model under different params\nalpha = .01    # learning rate\nbs = min(2**14, m//2)  # batch size: maxed at half size testset\nepochs = 400  # number of epochs per training round\nmodel.optimizer.lr = alpha","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45262d845573697904740953fbed911139e06a7e","_cell_guid":"02c45326-ab77-4ac3-8d08-5b6b936c6642","trusted":true},"cell_type":"code","source":"# evaluate model on training and validation sets\nprint(\"Loss by Mean Absolute Error\")\n\nprint(\"train:      \", model.evaluate(x=X,     y=y,     verbose=0, batch_size=bs))\nprint(\"validation: \", model.evaluate(x=X_val, y=y_val, verbose=0, batch_size=bs))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb0ab186bbf856181584a6c41609f9884fd2cbf4","_cell_guid":"2a85f91c-3bfa-4db5-9ae0-7f47cc04157b"},"cell_type":"markdown","source":"# start training","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0d3ad1ed29b2ddda056f028c806abb49ee4ef22d","collapsed":true,"_cell_guid":"3b56a94e-a08b-4fcd-9053-49e9a2cc9af9","trusted":true},"cell_type":"code","source":"timestart = datetime.datetime.now()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"925cf4e6f56d59d19519fc800f0990800b83ce36"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"36adb66470ba4ade2b76c2023760f2059dbc28bd","_cell_guid":"fcd69f31-9ccd-46b5-9232-24289ecf66d0","trusted":true},"cell_type":"code","source":"print(\"Learning Rate: %f, BatchSize: %i \"% (alpha, bs))\n# first we train one epoch to get the ugly random result out of my pretty graphs\nmodel.fit(X,y, epochs=1, verbose=1, batch_size=bs)\n\n# history = model.fit(X,y,batch_size=bs,epochs=epochs, validation_split=0.1, verbose =1)\nhistory = model.fit(X,y, batch_size=bs, epochs=epochs, validation_data=(X_val, y_val), verbose=0)\n\nloss     = int(model.evaluate(x=X,     y=y,     verbose=0, batch_size=bs))\nloss_val = int(model.evaluate(x=X_val, y=y_val, verbose=0, batch_size=bs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"798a18554ebc4a24178d853c27e47fd7bb920a4e"},"cell_type":"code","source":"loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"293456b85d1d0c4c8fbd23025fc9044f73bfa7e0","_cell_guid":"529615ca-9dce-429f-948e-8ddf5f705f0a","trusted":true},"cell_type":"code","source":"fname = f\"model_feature_redux_loss_{loss_val}.h5\"\nmodel.save(fname, include_optimizer=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bb2f5b1cddc4e6eecf0afcd0ac5251cc0f58634","_cell_guid":"f88e1f1a-e382-4f26-9496-2d3f6e511ad2","trusted":true},"cell_type":"code","source":"timedone = datetime.datetime.now()\nruntime = timedone - timestart\nprint(runtime)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1cbd935fc7f628a5a0eed0accd537ece64da37","_cell_guid":"d85e1b44-5c10-403d-b511-75914756e5d1"},"cell_type":"markdown","source":"# stop training","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a3e2360547ebdd08f83c4a6b6c1ccc99b4be960e","_cell_guid":"b46649c8-8da6-456e-8a6e-3221e6d82ac3","trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.title('model loss (mae)')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45262d845573697904740953fbed911139e06a7e","_cell_guid":"02c45326-ab77-4ac3-8d08-5b6b936c6642","trusted":true},"cell_type":"code","source":"# evaluate model on training and validation sets\nprint(\"Loss by Mean Absolute Error\")\n\nprint(\"train:      \", model.evaluate(x=X,     y=y,     verbose=0, batch_size=bs))\nprint(\"validation: \", model.evaluate(x=X_val, y=y_val, verbose=0, batch_size=bs))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8eab37bd0fcc3601e32789f8f0a3cc84ba23d10","_cell_guid":"757177fd-ead8-4c1c-b05c-7afa048b00f6"},"cell_type":"markdown","source":"## train some more, without validation data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"151c97d591ccaf932a6433a27dd629fb4966c6e1","_cell_guid":"c5833399-1f27-4742-98b5-14e81bceb88e","trusted":true},"cell_type":"code","source":"# reload train from df\ntrain = df.iloc[:len_train]\nX_labels = train.drop('Weekly_Sales',axis=1).columns.astype(str)\nX = train.drop('Weekly_Sales',axis=1).values\ny = train['Weekly_Sales'].values\n\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n# X=X_train\n# y=y_train\nprint(X.shape)\n# print(X_val.shape)\n# m_val,n_val = X_val.shape\nm,n = X.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeb75625a4b039b6db5bafdeb14db7ab79ce7635","collapsed":true,"_cell_guid":"b037f095-4de1-44f3-8d54-4aca0c645395","trusted":true},"cell_type":"code","source":"# these hyper parameters can be tweaked after compiling the model\n# this is useful for retraining an existing model under different params\nalpha = .001    # learning rate\nbs = min(2**14, m//2)  # batch size: maxed at half size testset\nepochs = 300  # number of epochs per training round\nmodel.optimizer.lr = alpha","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"36adb66470ba4ade2b76c2023760f2059dbc28bd","_cell_guid":"fcd69f31-9ccd-46b5-9232-24289ecf66d0","trusted":true},"cell_type":"code","source":"print(\"Learning Rate: %f, BatchSize: %i \"% (alpha, bs))\n# history = model.fit(X,y,batch_size=bs,epochs=epochs, validation_split=0.1, verbose =1)\nhistory = model.fit(X,y, batch_size=bs, epochs=epochs, verbose=0)\nloss     = int(model.evaluate(x=X,     y=y,     verbose=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"293456b85d1d0c4c8fbd23025fc9044f73bfa7e0","_cell_guid":"529615ca-9dce-429f-948e-8ddf5f705f0a","trusted":true},"cell_type":"code","source":"fname = f\"model_feature_redux_loss_{loss}.h5\"\nmodel.save(fname, include_optimizer=False)\n! ls -l *h5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45262d845573697904740953fbed911139e06a7e","_cell_guid":"02c45326-ab77-4ac3-8d08-5b6b936c6642","trusted":true},"cell_type":"code","source":"# evaluate model on training and validation sets\nprint(\"Loss by Mean Absolute Error\")\n\nprint(\"train:      \", loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bb2f5b1cddc4e6eecf0afcd0ac5251cc0f58634","_cell_guid":"f88e1f1a-e382-4f26-9496-2d3f6e511ad2","trusted":true},"cell_type":"code","source":"timedone = datetime.datetime.now()\nruntime = timedone - timestart\nprint(runtime)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f1cbd935fc7f628a5a0eed0accd537ece64da37","_cell_guid":"d85e1b44-5c10-403d-b511-75914756e5d1"},"cell_type":"markdown","source":"# stop 2nd training","outputs":[],"execution_count":null},{"metadata":{"_uuid":"a3e2360547ebdd08f83c4a6b6c1ccc99b4be960e","_cell_guid":"b46649c8-8da6-456e-8a6e-3221e6d82ac3","trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.title('model loss (mae)')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46158eb7ba12be1d100c8826e21a2b2d0263c95f","_cell_guid":"00ed8a22-9117-4de2-bc22-901bfcab7a60"},"cell_type":"markdown","source":"## predict for submission","outputs":[],"execution_count":null},{"metadata":{"_uuid":"79a5eed1275737fec7e776ed73bc3e7ddc056d61","collapsed":true,"_cell_guid":"fb3b8bbf-bc56-47df-a8d9-aaf512acbdb2","trusted":true},"cell_type":"code","source":"X_test = test.values\ny_pred = model.predict(X_test)  # ,batch_size=bs\ntestfile = pd.read_csv('../input/course-material-walmart-challenge/test.csv')\nsubmission = pd.DataFrame({'id':testfile['Store'].map(str) + '_' + testfile['Dept'].map(str) + '_' + testfile['Date'].map(str),\n                          'Weekly_Sales':y_pred.flatten()})\nfname = f\"submission_features_redux2_L{loss}.csv\"\nsubmission.to_csv(fname, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52e55b27d39df5d049d36998601ff89c4b655574","_cell_guid":"33aed7c2-51ba-439e-9543-618ed5f45029","scrolled":true,"trusted":true},"cell_type":"code","source":"! ls -l\n! wc -l submission*.csv","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"8a5393a5905db625bad683f9b4fcb5fb8635eb64","_cell_guid":"8cc189f0-b870-4ea7-a4f6-cd3a158885e9","trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(14,5))\nax.scatter(range(len(y_pred)), y_pred, alpha=.05, c='m')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"_uuid":"c943b95975f9208eb2074bc63a5e0503ed6f7480","_cell_guid":"c1c1e3a4-0fa9-4700-b041-a7b90f4a1014","trusted":true},"cell_type":"code","source":"inputlayer=True\nfor layer in model.layers:\n    if type(layer) == keras.layers.core.Dense:\n        weights = layer.get_weights()\n        fig,ax = plt.subplots( figsize=(5,5))\n        layer_img = np.vstack((weights[1], weights[0]))\n        plt.imshow(layer_img, cmap='PiYG')\n        \n        if inputlayer:\n            labels = X_labels.insert(0, 'Bias')\n            ax.set_yticklabels(labels)\n            ax.set_yticks(range(len(labels)))\n            inputlayer=False\n        else:\n            labels = np.array(range( 1, len(weights[0])+1 ))\n            labels = np.hstack((['Bias'], labels))\n            ax.set_yticklabels(labels)\n            ax.set_yticks(range(len(labels)))\n        plt.colorbar()\n        plt.title(layer.name)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"075c266d26b73b46c9a912f061f711df22963041","_cell_guid":"b42f3245-8aa5-4445-9307-96dfa795e0cc","trusted":true},"cell_type":"code","source":"for layer in model.layers:\n    weights = layer.get_weights()\n    print (weights)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc8475b58cff860ecb2b222ed049c87157842f18","collapsed":true,"_cell_guid":"dfc2cd2b-6723-4c01-a792-85ab5e19ec87","trusted":true},"cell_type":"code","source":"predicted = testfile.copy()\npredicted['Weekly_Prediction'] = y_pred\npredicted = pd.concat([predicted,pristine],axis=0) # Join predicted and pristine\n# predicted = pd.concat([predicted,df],axis=0) # Join predicted and pristine\npredicted['Date'] = predicted['Date'].astype('datetime64[ns]')\npredicted.set_index(['Date'], inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0071029a653078d21734cf41498c21fe0439c9a","collapsed":true,"_cell_guid":"1eb50987-eb3a-414a-ba30-7340bd87b924","trusted":true},"cell_type":"code","source":"# plots sales and predicted sales in time for specific Store/Dept\ndef plot_store_dept_pred(plotme):\n    wpm = int(plotme.Weekly_Prediction.mean())\n    wsm = int(plotme.Weekly_Sales.mean())\n    fig, ax = plt.subplots(figsize=(13,4))\n    ax.set_ylabel(\"Sales\")\n    ax.bar(x=plotme.index, height=plotme[\"Weekly_Sales\"],width=7, color='g', label=\"known sales\")\n    ax.axhline(y=wsm, c='g', label=f\"known sales mean {wsm}\")\n    ax.bar(x=plotme.index, height=plotme[\"Weekly_Prediction\"],width=5, \n           label=\"predicted sales\", color='g', alpha=.5)\n    ax.axhline(y=wpm, c='c', label=f\"predicted sales mean {wpm}\")\n    ax.plot_date(x=plotme.index, y=(plotme[\"IsHoliday\"] * wsm), fmt='*m')\n    ax.legend()\n    plt.title(f\"Store: {store}\\nDept: {dept}\")\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07582274767bba9a26847c7d638dbc5ceea3092a","collapsed":true,"_cell_guid":"239e46ef-a7b5-4d71-8238-b22dfcce9685","trusted":true},"cell_type":"code","source":"store=9\ndept=30","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"33de4e324596d2448d8067a3f88eafd09252bf1d","_cell_guid":"225dd30c-4a42-47ac-aa76-79f615828f6d","trusted":true},"cell_type":"code","source":"# store += 1\ndept  += 1\n\nwhere = (predicted['Store'] == store) & (predicted['Dept'] == dept)\nselect = ['Date', 'Weekly_Prediction', 'Weekly_Sales', 'IsHoliday']\nplotme = predicted.loc[where, select]\nplot_store_dept_pred(plotme)\n# print(plotme.Weekly_Prediction.mean(),plotme.Weekly_Sales.mean())\n# print(plotme.Weekly_Prediction.count(),plotme.Weekly_Sales.count())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"763092f6c532915924d54070c7f3ee4317ec0c2e","collapsed":true,"_cell_guid":"c1ef9703-0fb8-463b-bce6-08818d88ab0e","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efe249658de56a51ff663cef9ff8aa105dcbfc16","collapsed":true,"_cell_guid":"1852bf03-31b6-4289-bd35-aa6b6fd7b9a4","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d5ab63a40430729be00f0b4c2cb1fcc167ecd5c","collapsed":true,"_cell_guid":"23a681a7-9558-4cb2-91b9-eb5d7f416f75","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15ab0ef27024b42c39b4d9198d08abff34627716","collapsed":true,"_cell_guid":"2fc73962-124f-442f-a459-3f9860a39ee9","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb3b6428a4c19816df15a3bd36830f503159f47a","collapsed":true,"_cell_guid":"e9ff2d19-fe2c-4692-9f4a-1ecb9d73da47","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5deb68d53d8e6b96180be4a9d754d09b0d7882fb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"82eb35ba8c93b9e152f33aefe1b7316c486ed63a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"574e4b1889ded5cac82eda32e41cd056afb18c3b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ead9569171adca74390d4954f66a13102a6365dd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}