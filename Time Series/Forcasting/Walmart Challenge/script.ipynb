{"cells":[{"metadata":{"_uuid":"cd3296c40587f51a36e1818ae4a310ad05b658b5","_cell_guid":"7255a69e-6c53-434f-829d-405ecb3798e0"},"cell_type":"markdown","source":"# The Walmart challenge: Modelling weekly sales\nIn this notebook, we use data from Walmart to forecast their weekly sales. "},{"metadata":{"collapsed":true,"_uuid":"5c7360ce9d2ba20f564c8c230f4ab44b2b849ea3","_cell_guid":"dfc3c722-10c5-46fc-9f8b-c078d99205b8"},"cell_type":"markdown","source":"## Summary of results and approach\n\nWork in Progress:\n\nAt writing, our internal competition at Bletchley has ended. Interestingly, the winning group had a different approach then would be expected from an AI/Machine Learning bootcamp. Their forecasts were based simply on a median of the weekly sales grouped by the Type of Store, Store & Department number, Month and Holiday dummy. \n\nTherefore, in my next approach, the goal will be to improve their results with the help of Neural Networks. In fact, the median will be computed similarly to how the winning group did, and a new variable, the difference to the median, will be computed. This difference will be the new dependent variable and will be estimated based on new holiday dummies, markdowns and info on lagged sales data if available.\n\n**Unfortunately, it appears that so far the models do not find any possible improvements over the median sales forecasts with the available explanatory variables.\n**"},{"metadata":{"_uuid":"dbb52c9e2a03a83255ba903f498092e9f590ac9a","_cell_guid":"014a771f-ba58-4183-8ca6-301d9c2aa97f"},"cell_type":"markdown","source":"## Understanding the problem and defining a success metric\n\nThe problem is quite straightforward. Data from Walmart stores accross the US is given, and it is up to us to forecast their weekly sales. The data is already split into a training and a test set, and we want to fit a model to the training data that is able to forecast those weeks sales as accurately as possible. In fact, our metric of interest will be the [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error). \n\nThe metric is not very complicated. The further away from the actual outcome our forecast is, the harder it will be punished. Optimally, we exactly predict the weekly sales. This of course is highly unlikely, but we must try to get as close as possible. The base case of our model will be a simple linear regression baseline, which gave a MSE of \n\n"},{"metadata":{"_uuid":"4a4ac64d7e90f02d683685cf9c9432ebb56e2580","_cell_guid":"c6b58708-9f9a-42ee-a437-d8ffa169cde0"},"cell_type":"markdown","source":"## Load and explore data\nBefore we do anything, lets import some packages."},{"metadata":{"collapsed":true,"_uuid":"3f34f97043b07eb2c0cca8fdc317a199ace93a6c","_cell_guid":"7c399d8c-5531-44a8-a758-ef7785518f28","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom string import ascii_letters\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime, timedelta","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"3d232efd3a8d58731842ed29f6e4488917204082","_cell_guid":"5b80c98a-8fbc-47b8-92ba-718c0be74dd1"},"cell_type":"markdown","source":"Now, load the train and test data."},{"metadata":{"collapsed":true,"_uuid":"03a0a3ff6dfdb3d53a037d727f6260c0412990a8","_cell_guid":"ecb87ead-f34a-4449-95f9-2c9c0764247d","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"cca3344c7d9c45a2c0dc41fe90e88083e15fb8de","_cell_guid":"04fef74f-4f53-4328-a882-7e3be34c386b"},"cell_type":"markdown","source":"In order to efficiently modify our data, we merge the two datasets for now. We also keep track of the length of our training set so we know how to split it later."},{"metadata":{"_uuid":"e6c57c708a9db7fc01447b6b00908b67cb7f4b09","_cell_guid":"c813405d-10a0-45ba-891e-e4e494ab34fc","trusted":true},"cell_type":"code","source":"t_len = len(train) # Get number of training examples\ndf = pd.concat([train,test],axis=0) # Join train and test\ndf.head() # Get an overview of the data","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"adac71f84a15958f3ec618c5c49f8f86acbb9d06","_cell_guid":"8bca733e-3ba3-4db2-bcc1-66b371f702ab"},"cell_type":"markdown","source":"Let's get a clearer image of what our data actually looks like with the describe function. This will give use summary statistics of our numerical variables."},{"metadata":{"_uuid":"b2d03ef65c79c4144d743e6ab70f2de365a26c61","scrolled":true,"_cell_guid":"7094d024-8782-4d1c-8b46-ddf7140f46ab","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"db86071226556482ae6dbc13cc3f613c3539a44e","_cell_guid":"4e5f019b-4264-4d27-9b69-3b3b57d16e87"},"cell_type":"markdown","source":"Since we are in the Netherlands, and we don't understand Fahrenheit, let's do a quick change there."},{"metadata":{"collapsed":true,"_uuid":"25400d3e46ccfe5172b8588f1cf492190daede13","_cell_guid":"c511049e-674b-4602-9393-5b0a3c38d616","trusted":true},"cell_type":"code","source":"df['Temperature'] = (df['Temperature'] - 32) * 5/9","execution_count":5,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"c39ebb82e23003d33d29b408adbe67bddc65cde0","_kg_hide-output":false,"_cell_guid":"ff58816e-5ed2-4e08-982a-0b5508f26f4e"},"cell_type":"markdown","source":"Although there is not a large variety of variables, we can definitely work with this. In the next section, we will clean the data set, engineer some new features and add dummy variables. For now, let's try to find any obvious relations between our variables to get a feeling for the data. We begin with a correlation matrix.\n"},{"metadata":{"_uuid":"4af4dc2d78fa747d7a835773e35048f7dbb401fb","_cell_guid":"4abbc5ad-f380-429e-b8d8-10917e8961ec","trusted":true},"cell_type":"code","source":"# Code from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"1c22efea7c198086cf23322fa0209b9569b15e68","_cell_guid":"eded2bed-76d0-431c-9163-1ade898383fc"},"cell_type":"markdown","source":"Most of what we see in the correlation table is of little surprise. Discounts are correlated and higher unemployment means lower Consumer Price Index. More interestingly, it appears that higher department numbers have higher sales. Maybe because they are newer? Also, larger stores generate more sales, discounts generally generate higher sales values and larger unemployment result in a bit fewer sales. Unfortunately, there appears to be little relationship between holidays, temperatures or fuelprices with our weekly sales.\n\nNext up, let's plot some of these relationships to get a clearer image."},{"metadata":{"_uuid":"4e438a2c22546a83c92950ddbd9921ce0763b99e","_cell_guid":"d28d3d3d-dea6-4dd6-8f85-bbc47c6d9c89","trusted":true},"cell_type":"code","source":"%matplotlib inline\ndef scatterplots(feature, label):\n    x = feature\n    y = df['Weekly_Sales']\n    plt.scatter(x, y)\n    plt.ylabel('sales')\n    plt.xlabel(label)\n    plt.show()\n\nheaders = list(df)\nlabels = headers\nscatterplots(df['Fuel_Price'], 'Fuel_Price')\nscatterplots(df['Size'], 'Size')\nscatterplots(df['Temperature'], 'Temperature')\nscatterplots(df['Unemployment'], 'Unemployment')\nscatterplots(df['IsHoliday'], 'IsHoliday')\nscatterplots(df['Type'], 'Type')\n\n","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"4a892b0728f61c43132b845fd2ca772fa564598c","_cell_guid":"14c68f69-2452-4078-95e1-013515ecc760"},"cell_type":"markdown","source":"From this plot, we notice that type C stores have fewer sales in general and holidays clearly show more sales.Although no further relationships appear evident from this analysis, there appears to be some outliers in our data. Let's take a bit of a closer look at these."},{"metadata":{"_uuid":"a302b87ee2f762c618cf068e3bd085f58143e1e3","_cell_guid":"dda5531a-c622-409e-8c7c-852fbea528d2","trusted":true},"cell_type":"code","source":"df.loc[df['Weekly_Sales'] >300000]","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"d14d5237ba53fc3181f51b1c15d4bdcf9239549c","_cell_guid":"5eae0c43-ccfa-4e1b-97e8-d457f2bf5eee"},"cell_type":"markdown","source":"It appears to be quite obvious. The end of November sees a lot of exceptionally large sales. This special day, better known as Black friday, causes sales to be on fire, and undoubtedly a dummy variable should be created for this day. Also, Christmas, appears here and there. Since it is not considered holiday, we will also make a dummy for this day. Let's see if we should consider some other special days as well."},{"metadata":{"_uuid":"c2d02c25b2f203cf39b29de7c58f70f8881042d2","scrolled":true,"_cell_guid":"dcf43c7b-2bc3-4bbc-a28c-2145c909ef55","trusted":true},"cell_type":"code","source":"df.loc[df['Weekly_Sales'] >240000,\"Date\"].value_counts()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"5111380a08a7718088eecf12e3c1a9017e67c5ea","_cell_guid":"688a6ff2-8b99-47d6-b622-c98a726de744"},"cell_type":"markdown","source":"Except for a handful spurious other dates, it appears that the two days before Christmas and Black Friday will do the job."},{"metadata":{"_uuid":"f774dcb9a50a4dea6ae2042411f4a5c6ce59a667","_cell_guid":"39f735a8-adfd-4e56-9974-4425a5e482d7"},"cell_type":"markdown","source":"\n\n## Scrub the data and engineer features\n\n### Missing values\n\nWe will start with filling in any blank values. There seem to be some missing values in the data. We have to make sure to deal with them before feeding anything into the network."},{"metadata":{"_uuid":"69a906f1e30491b5e20eadc6a807862ee4c4a344","scrolled":true,"_cell_guid":"4c2f78ee-4bf8-4d90-9e61-611aed761c1b","trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"51e0995047f008d796329e30e1b8d38e8040c124","_cell_guid":"703abe3a-cbd5-4b81-8751-92c11d85d489"},"cell_type":"markdown","source":"We will do a bit of very basic feature engineering here by creating a feature which indicates whether a certain markdown was active at all."},{"metadata":{"collapsed":true,"_uuid":"b9a1c793ded40b597f959abcaaf8cebec9ee0423","_cell_guid":"90dbdf5c-2de5-493b-a846-3db0e9a2641e","trusted":true},"cell_type":"code","source":"df = df.assign(md1_present = df.MarkDown1.notnull())\ndf = df.assign(md2_present = df.MarkDown2.notnull())\ndf = df.assign(md3_present = df.MarkDown3.notnull())\ndf = df.assign(md4_present = df.MarkDown4.notnull())\ndf = df.assign(md5_present = df.MarkDown5.notnull())","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"9f8d42ccef4c37705e8dc64fafb64e3a45104473","_cell_guid":"387023f1-dc2b-4b54-8483-6401d24b0d64"},"cell_type":"markdown","source":"We can probably safely fill all missing values with zero. For the markdowns this means that there was no markdown. For the weekly sales, the missing values are the ones we have to predict, so it does not really matter what we fill in there."},{"metadata":{"collapsed":true,"_uuid":"5868b63dfc643d6d8015267c10c8105357de18fc","_cell_guid":"690b5760-b2c9-4471-8355-fc9b3b0e0d73","trusted":true},"cell_type":"code","source":"df.fillna(0, inplace=True)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"2401b7681253b94de41c9cbbda369591cc68918e","_cell_guid":"a237b1b4-8001-4ad3-a8b1-784f407fb768"},"cell_type":"markdown","source":"### Dummy variables: Categorical Data\n\nNow we have to create some dummy variebles for categorical data."},{"metadata":{"collapsed":true,"_uuid":"98780a9a5fe53e4944b96cc6417b3c23331fa6ce","_cell_guid":"75c044d2-4552-4371-91ce-563e8f2f305d","trusted":true},"cell_type":"code","source":"# Make sure we can later recognize what a dummy once belonged to\ndf['Type'] = 'Type_' + df['Type'].map(str)\ndf['Store'] = 'Store_' + df['Store'].map(str)\ndf['Dept'] = 'Dept_' + df['Dept'].map(str)\ndf['IsHoliday'] = 'IsHoliday_' + df['IsHoliday'].map(str)","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e4eb22919e3d570949db155871eb1f459b218348","_cell_guid":"9501b560-9dd0-4cbc-9ccd-b517c54c1e58","trusted":true},"cell_type":"code","source":"# Create dummies\ntype_dummies = pd.get_dummies(df['Type'])\nstore_dummies = pd.get_dummies(df['Store'])\ndept_dummies = pd.get_dummies(df['Dept'])\nholiday_dummies = pd.get_dummies(df['IsHoliday'])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"ced7e187ea4435f8ac2a09368a4545342b787760","_cell_guid":"3a8c1806-f457-4711-8e8e-11dcbbdb32ab"},"cell_type":"markdown","source":"### Dummy variables: Dates\n\nFrom our earlier analysis, it has turned out that the date may be our best friend. As a general rule, it is a good start to already distinguish between different months in our model. This will create 12 dummy variables; one for each month."},{"metadata":{"collapsed":true,"_uuid":"f95a5a2926c412f0da7cbf1ffd5257031fa67130","_cell_guid":"67cbfda4-1074-498f-9f0e-abb41523807f","trusted":true},"cell_type":"code","source":"df['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in df['Date'].astype(str).values.tolist()]\ndf['Month'] = [date.month for date in df['DateType']]\ndf['Month'] = 'Month_' + df['Month'].map(str)\nMonth_dummies = pd.get_dummies(df['Month'] )","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"af711b6e8fc0d29439122879d2d0ca3cdb16b6a5","_cell_guid":"37956ac5-0053-43c5-ad33-50964fdb6ea4"},"cell_type":"markdown","source":"Next, let's look at 'special dates'. One variable for Christmas, one for black friday. We have to manually look up the dates of black friday if we want to extrapolate our data to other years, but for now we know: 26 - 11 - 2010 and 25 - 11 - 2011."},{"metadata":{"collapsed":true,"_uuid":"ab8bb7940ae5a6ebcca99cb8e375ceb2d80f8b19","_cell_guid":"7e7dae6b-5180-463b-9d96-27e69a8f6f89","trusted":true},"cell_type":"code","source":"df['Black_Friday'] = np.where((df['DateType']==datetime(2010, 11, 26).date()) | (df['DateType']==datetime(2011, 11, 25).date()), 'yes', 'no')\ndf['Pre_christmas'] = np.where((df['DateType']==datetime(2010, 12, 23).date()) | (df['DateType']==datetime(2010, 12, 24).date()) | (df['DateType']==datetime(2011, 12, 23).date()) | (df['DateType']==datetime(2011, 12, 24).date()), 'yes', 'no')\ndf['Black_Friday'] = 'Black_Friday_' + df['Black_Friday'].map(str)\ndf['Pre_christmas'] = 'Pre_christmas_' + df['Pre_christmas'].map(str)\nBlack_Friday_dummies = pd.get_dummies(df['Black_Friday'] )\nPre_christmas_dummies = pd.get_dummies(df['Pre_christmas'] )","execution_count":16,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d2a1eb2f06304e674d02fa4a849a4ccf6dfcf08d","_cell_guid":"a074776c-7611-47e9-93dc-3670a93bfb4e","trusted":true},"cell_type":"code","source":"# Add dummies\n# We will actually skip some of these\n#df = pd.concat([df,type_dummies,store_dummies,dept_dummies,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies,Month_dummies],axis=1)\n\ndf = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"c0cc9cde9f9dfae6da25336c04c79e1d45d6459e","_cell_guid":"382a1a31-2659-437a-84d7-999385f894c6"},"cell_type":"markdown","source":"> ### Store median\n\nWe will take the store median in the available data as one of its properties"},{"metadata":{"collapsed":true,"_uuid":"4ae0d5448ea79dd51133285114ef66763e8c3aeb","_cell_guid":"86366382-9a01-430f-ab80-8c509b388265","trusted":true},"cell_type":"code","source":"# Get dataframe with averages per store and department\nmedians = pd.DataFrame({'Median Sales' :df.iloc[:282451].groupby(by=['Type','Dept','Store','Month'])['Weekly_Sales'].median()}).reset_index()\n","execution_count":18,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f1f136314bc5a9bf038a7db33266a09822a44673","_cell_guid":"52c88d6f-c069-415d-9acd-619cacdfe50a","trusted":true},"cell_type":"code","source":"\n# Merge by type, store, department and month\ndf = df.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month'])\n","execution_count":19,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c23b9f955d3047468135eb95040bd13e1d7e4a35","_cell_guid":"1fae052c-9995-4ac4-8dc8-2fff168cee6e","trusted":true},"cell_type":"code","source":"# Fill NA\ndf['Median Sales'].fillna(df['Median Sales'].iloc[:282451].median(), inplace=True) \n\n# Create a key for easy access\n\ndf['Key'] = df['Type'].map(str)+df['Dept'].map(str)+df['Store'].map(str)+df['Date'].map(str)\n","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"f538b52a36a0a045a49c68478f0f39dd3c27cdda","_cell_guid":"7caa328a-aa5c-43e0-9d9c-8ddfbfe2cfdd","trusted":true},"cell_type":"code","source":"df.head()","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"d475888086c9354900408b59e79458957fa9f8ef","_cell_guid":"882024a8-8579-4c12-926d-e24790e8e7a9"},"cell_type":"markdown","source":"### Lagged Variables\n\nWe will take a lagged variable of our store's previous weeks sales. To do so, we will first add a column with a one week lagged date, sort the data, and then match the lagged sales with the initial dataframe using the department and store number.\n\nWe begin by adding a column with a one week lag."},{"metadata":{"_uuid":"1db8e347f7dcc3cfe4d4b8499d55c8ba5eca622d","_cell_guid":"04979adf-e73f-4c3c-ad82-99394cf9c848","trusted":true},"cell_type":"code","source":"# Attach variable of last weeks time\ndf['DateLagged'] = df['DateType']- timedelta(days=7)\ndf.head()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"17d292e1e0c9a34ab39b7248bf33bb24e0731c51","_cell_guid":"d4dc55db-5698-4c53-9129-e4d176651d36"},"cell_type":"markdown","source":"Next, we create a sorted dataframe."},{"metadata":{"collapsed":true,"_uuid":"fd8fa3b6b46192d1da45e28cfd964f3e98fa2b6b","_cell_guid":"656777c7-28f9-4bbc-832e-437421ce60ff","trusted":true},"cell_type":"code","source":"# Make a sorted dataframe. This will allow us to find lagged variables much faster!\nsorted_df = df.sort_values(['Store', 'Dept','DateType'], ascending=[1, 1,1])\nsorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"057d70644bdbc25238f26bb48d57c467940abc5e","_cell_guid":"1e289d20-9327-45d5-890d-db045ee4af1c"},"cell_type":"markdown","source":"Loop over its rows and check at each step if the previous week's sales are available. If not, fill with store and department average, which we retrieved before."},{"metadata":{"_uuid":"ea79299edffdfaa429625e01835c95e6f367f19f","scrolled":true,"_cell_guid":"31370ac7-727d-4a46-9543-d9cee49716ef","trusted":true},"cell_type":"code","source":"sorted_df['LaggedSales'] = np.nan # Initialize column\nlast=df.loc[0] # intialize last row for first iteration. Doesn't really matter what it is\nrow_len = sorted_df.shape[0]\nfor index, row in sorted_df.iterrows():\n    lag_date = row[\"DateLagged\"]\n    # Check if it matches by comparing last weeks value to the compared date \n    # And if weekly sales aren't 0\n    if((last['DateType']== lag_date) & (last['Weekly_Sales']>0)): \n        sorted_df.set_value(index, 'LaggedSales',last['Weekly_Sales'])\n    else:\n        sorted_df.set_value(index, 'LaggedSales',df['Median Sales'].loc[index]) # Fill with median\n\n    last = row #Remember last row for speed\n    if(index%int(row_len/10)==0): #See progress by printing every 10% interval\n        print(str(int(index*100/row_len))+'% loaded')","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"0c383fdd3e1e2989217fb3e2a8d0aeb9acbdb6b1","_cell_guid":"40a5ab19-9277-44b5-ac17-7d99c888b9c0","trusted":true},"cell_type":"code","source":"sorted_df.head()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"b9742e76e055bd2f25d8e97cf75fdffeda13b7dd","_cell_guid":"85debbad-7690-45a9-a307-091092c21544"},"cell_type":"markdown","source":"Now, merge this new info with our existing dataset."},{"metadata":{"collapsed":true,"_uuid":"a1c186a16d976b3eaa7e87697f708c41ee31814b","scrolled":true,"_cell_guid":"b35ee87d-f3e7-44e8-bc77-c2b959b15d18","trusted":true},"cell_type":"code","source":"# Merge by store and department\ndf = df.merge(sorted_df[['Dept', 'Store','DateType','LaggedSales']], how = 'inner', on = ['Dept', 'Store','DateType'])","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"ed8c998963b12a949be33d494d5b541509ea7170","_cell_guid":"026b4c48-8f6b-4ea9-82b3-17444f916d52"},"cell_type":"markdown","source":"### Remove redundant items\n\nWe will take the store average in the available data as one of its properties"},{"metadata":{"collapsed":true,"_uuid":"758e0d147bfcf275fe8b5ce36606cb826d81decd","_cell_guid":"49075c4e-6157-4214-ae2b-9abbde3021c7","trusted":true},"cell_type":"code","source":"# Remove originals\ndel df['Type']\ndel df['Store']\ndel df['Dept']\ndel df['IsHoliday']\ndel df['DateType']\ndel df['Date']\ndel df['Month']\ndel df['Pre_christmas'] \ndel df['Black_Friday']\ndel df['DateLagged']\ndel df['Key']","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"aa28b9433c8f292436ddaa4dbae89798b67c353e","_cell_guid":"9e30259c-a9c9-4512-8110-c3552e70f6a9"},"cell_type":"markdown","source":"### Scale Variables\n\nTo make the job of our models easier in the next phase, we normalize our continous data. This is also called feature scaling."},{"metadata":{"collapsed":true,"_uuid":"bf227383541055c70aee4769c6c5ffd62a7eff1e","_cell_guid":"d614753c-3086-4a2e-9c68-8f6dc795859c","trusted":true},"cell_type":"code","source":"df['Unemployment'] = (df['Unemployment'] - df['Unemployment'].mean())/(df['Unemployment'].std())\ndf['Temperature'] = (df['Temperature'] - df['Temperature'].mean())/(df['Temperature'].std())\ndf['Fuel_Price'] = (df['Fuel_Price'] - df['Fuel_Price'].mean())/(df['Fuel_Price'].std())\ndf['CPI'] = (df['CPI'] - df['CPI'].mean())/(df['CPI'].std())\ndf['MarkDown1'] = (df['MarkDown1'] - df['MarkDown1'].mean())/(df['MarkDown1'].std())\ndf['MarkDown2'] = (df['MarkDown2'] - df['MarkDown2'].mean())/(df['MarkDown2'].std())\ndf['MarkDown3'] = (df['MarkDown3'] - df['MarkDown3'].mean())/(df['MarkDown3'].std())\ndf['MarkDown4'] = (df['MarkDown4'] - df['MarkDown4'].mean())/(df['MarkDown4'].std())\ndf['MarkDown5'] = (df['MarkDown5'] - df['MarkDown5'].mean())/(df['MarkDown5'].std())\ndf['LaggedSales']= (df['LaggedSales'] - df['LaggedSales'].mean())/(df['LaggedSales'].std())","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"a85252d8752600c322b68945092d7aac63f932ff","_cell_guid":"a6f2538f-4ea4-47e3-aed4-63196f658d0c"},"cell_type":"markdown","source":"Now, let's change the variable to be forecasted to the difference from the median. Afterward, we can drop the weekly sales."},{"metadata":{"collapsed":true,"_uuid":"38619f3351d17fbeb5da64766aca53e78b24f4aa","_cell_guid":"e8179fdb-8c6e-4f54-9444-2d8a3447cefe","trusted":true},"cell_type":"code","source":"df['Difference'] = df['Median Sales'] - df['Weekly_Sales']\ndel df['Weekly_Sales'] ","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"91dfd50c5793354a48700e273c5659f9026e337c","_cell_guid":"d18c24ee-d81a-40c5-91dc-8095614ba040"},"cell_type":"markdown","source":"Let's have a look at our data set before running our actual models."},{"metadata":{"_uuid":"8104a375ba22abe1059ce4c90d5526ce69492e7a","_cell_guid":"9b8f5c99-fd52-4ab8-838d-b969648703d0","trusted":true},"cell_type":"code","source":"df.head()","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"83fb83fe08e7612d1a209fea36a1347ec46ebd84","_cell_guid":"702b605e-b4e4-4714-9816-f116bc26c0e0","trusted":true},"cell_type":"code","source":"# Code from https://seaborn.pydata.org/examples/many_pairwise_correlations.html\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"6b694a6d6061160086840fc79457581a1ae385cb","_cell_guid":"359a7deb-1b6c-4e46-8d22-e350a4a98f2e"},"cell_type":"markdown","source":"### Select variables to include in model\n\nIn this section, we can change the variables we ultimately want to include in our model training. "},{"metadata":{"collapsed":true,"_uuid":"45707cd233f4c182591d1a3adc13666db8ba3bb0","_cell_guid":"bc8a320e-7861-499e-b599-f5903a1f3650","trusted":true},"cell_type":"code","source":"selector = [\n    'CPI',\n    'Fuel_Price',\n    #'MarkDown1',\n    #'MarkDown2',\n    #'MarkDown3',\n    #'MarkDown4',\n    #'MarkDown5',\n    'Size',\n    #'Temperature',\n    #'Unemployment',\n\n    'md1_present',\n    'md2_present',\n    'md3_present',\n    'md4_present',\n    'md5_present',\n\n    'IsHoliday_False',\n    'IsHoliday_True',\n    'Pre_christmas_no',\n    'Pre_christmas_yes',\n    'Black_Friday_no',\n    'Black_Friday_yes',    \n    'LaggedSales'\n        ]","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"2880be4b30dad2cef5c6d92d0cbefe1cf8dde843","_cell_guid":"146c8730-4139-4a2c-8b7b-2878fcac8c9b"},"cell_type":"markdown","source":"### Split data into training and test sets\n\nNow we can split train test again and of course remove the trivial weekly sales data from the test set."},{"metadata":{"collapsed":true,"_uuid":"a929329c561f59e9ee2651aeb7f0247cbff2496b","_cell_guid":"99c55060-308e-4946-bb70-8d542a7d349f","trusted":true},"cell_type":"code","source":"train = df.iloc[:282451]\ntest = df.iloc[282451:]","execution_count":33,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"384792c7780c2fe3ebea7f988efeb749d5446688","_cell_guid":"ba7d8a64-6911-4cdd-b8fd-2e82abb0ee47","trusted":true},"cell_type":"code","source":"y = train['Difference'].values","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"a8b39cd5493a5d870050f7138f67564d8a39e223","_cell_guid":"e5843e97-a2f0-499f-bb5a-f76b5892cf67","trusted":true},"cell_type":"code","source":"X = train[selector].values\ntrain.head()","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"ff8bb740f2609051150f02018d4d7906c12a0df3","_cell_guid":"a233c643-a90a-43ac-b43f-4530b9f2e969"},"cell_type":"markdown","source":"### Test - dev\n\nUsually, model performance can be evaluated on the out-of-sample test set. However, since that data is not available, it may be wise to split our training set one more time in order to be able to test out of sample performance. Let's give up 20% of our training set for this sanity check development set."},{"metadata":{"collapsed":true,"_uuid":"41f4e4e3d4514e7a3de0552083c5c4fdf396c3d7","_cell_guid":"ede59414-26a2-4cda-b003-e5d0f862cf9f","trusted":true},"cell_type":"code","source":"# Set seed for reproducability \nnp.random.seed(42)\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"dd252e577ea69f0d9e2d187c00db6150388860e0","_cell_guid":"70f65c84-790c-41e9-93a9-4ead6c2ee676"},"cell_type":"markdown","source":"## Model selection\n\nAs usual, let's start off with all our imports."},{"metadata":{"_uuid":"e7f30cbfb0fdadcd71966f0bd9a21cbf8e6d97af","_cell_guid":"2432e904-f354-4d14-97e8-fd51482daa3a","trusted":true},"cell_type":"code","source":"# Get Keras\n# We will build a sequential model\nfrom keras.models import Sequential\n# Using fully connected layers\nfrom keras.layers import Dense, Activation\n# With vanilla gradient descent\nfrom keras.optimizers import SGD\n# Adam optimizer\nfrom keras.optimizers import adam as adams\n# Regulizer to avoid overfitting\nfrom keras import regularizers \n# Dropout to avoid overfitting\nfrom keras.layers import Dropout\nfrom keras import optimizers","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"ac62685aca98c722c3a3bdae9ee52a2da87bafc5","_cell_guid":"4aa178f0-92ed-49b2-b5ab-3037a3c7564f"},"cell_type":"markdown","source":"Set some parameters to be used by all models"},{"metadata":{"_uuid":"730a302d0dd6b043e92be36b55d49abbc7c23eec","_cell_guid":"d8e39c86-98b9-4bd2-bc57-62aec1d727d2","trusted":true},"cell_type":"code","source":"learning_rate = 0.1\nm,n = X.shape\n\n# these hyper parameters can be tweaked after compiling the model\n# this is useful for retraining an existing model under different params\nbatch_s = min(2**14, m//2)  # batch size: maxed at half size testset\nprint(n)\nepochs = 10  # number of epochs per training round","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"24590368b22d144f97fa0ca19d5e5648ef5fcea3","_cell_guid":"20f5eb5b-6250-4dc0-bd58-1e3909e9b645","trusted":true,"collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(65, activation='relu', input_dim=n))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1))\nadam=optimizers.Adam(lr=0.15, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(optimizer=adam, loss='mae')","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"bcb68eb0caec7d1edc4f89a01898e07a416d1a8b","_cell_guid":"9de8f686-c393-4fb4-84f5-7948cc6ac79e","trusted":true},"cell_type":"code","source":"Sequential_feature_scaled = model.fit(X_train,y_train,batch_size=2048,epochs=epochs)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"86039d7bd88588e363fedcaf89a4fcc5fc21a436","_cell_guid":"eee7cc17-eebf-477a-9ab1-e0ff12e0da7c"},"cell_type":"markdown","source":"### Linear Regression\n\nWe will start with the baseline linear regression model to use as a benchmark for more complicated models. Note that the loss function is defined to reflect the mean average error.\n\nWe will store the output of each trained model in a history retrieval variable."},{"metadata":{"_uuid":"6752c61ed7d3ef84d91aebfe9a47c157ba175b51","_cell_guid":"0eeff49e-68ad-434e-b06c-046df7aaf712","trusted":true},"cell_type":"code","source":"lin_reg = Sequential()\nlin_reg.add(Dense(1,input_dim=n))\nlin_reg.compile(optimizer='adam', loss='mean_absolute_percentage_error')\nhistory_lin_reg = lin_reg.fit(X_train,y_train,batch_size=batch_s,epochs=epochs,verbose=0)\nlin_reg.evaluate(x=X_dev,y=y_dev)","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"e72c356d09f67ae0884a2b990fbe1b9c5c095b55","_cell_guid":"39a40485-2fa0-4884-bb81-691d7fe5f8d6"},"cell_type":"markdown","source":"### Tanh activation\n\nWe begin with a fairly simple model, include a hidden tanh activation layer. This model should already be able to outperfom our basic linear regression model."},{"metadata":{"collapsed":true,"_uuid":"8cc1976a5fcf33bc3c64f6bfb3b187b2b577916c","_cell_guid":"688ff62e-e09f-48cb-ab1a-ec2f95a93a11","trusted":true},"cell_type":"code","source":"# Sequential model\ntanh_model = Sequential()\n\n# First hidden layer\ntanh_model.add(Dense(32,activation='tanh',input_dim=n))\n\n# Second hidden layer\ntanh_model.add(Dense(16,activation='tanh'))\n\n# Output layer\ntanh_model.add(Dense(1,activation='sigmoid'))\n\n# Compile the model\ntanh_model.compile(optimizer=SGD(lr=learning_rate),\n              loss='mean_squared_logarithmic_error',\n              metrics=['acc'])\n\n# Train\nhistory_tanh = tanh_model.fit(X_train, y_train, # Train on training set\n                         epochs=epochs, # We will train over 1,000 epochs\n                         batch_size=batch_s, # Batch size \n                         verbose=0) # Suppress Keras output\n","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"0f52b8a8fcb0c93389f6ec162d7d6aae3587cc27","scrolled":true,"_cell_guid":"e25a8dfb-4d74-44d9-96a1-d27f2b382732","trusted":true},"cell_type":"code","source":"tanh_model.evaluate(x=X_dev,y=y_dev)","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"0c5ec5f1538ec24b4e50f504db18edd597be6154","_cell_guid":"53e1f002-eec5-4314-a724-b53ffe33e0ce"},"cell_type":"markdown","source":"### Relu activation with momentum\n\nIn our next model, we replace the tanh function with the computationally more efficient Relu function. This should speed up the calculation of epochs and allow us to reduce our MSE faster. Furthermore, we will implement a moving average momentum element that smoothens out our gradient direction in its decent."},{"metadata":{"_uuid":"c23cd60cb90c4fde5e48e346843f2bfbdb6d083c","_cell_guid":"74e84e4f-543d-422d-85f7-58b3a5487966","trusted":true},"cell_type":"code","source":"# Sequential model\nrelu_momentum = Sequential()\n\n# First hidden layer\nrelu_momentum.add(Dense(32,activation='relu',input_dim=n))\n\n# Output layer\nrelu_momentum.add(Dense(1,activation='sigmoid'))\n\n# Setup optimizer with learning rate of 0.01 and momentum (beta) of 0.9\nmomentum_optimizer = SGD(lr=learning_rate, momentum=0.9)\n\n# Compile the model\nrelu_momentum.compile(loss='kullback_leibler_divergence', optimizer=momentum_optimizer)\n# Train\nhistory_relu_momentum = relu_momentum.fit(X_train, y_train, # Train on training set\n                             epochs=epochs, # We will train over 1,000 epochs\n                             batch_size=batch_s, # Batch size \n                             verbose=0) # Suppress Keras output\nrelu_momentum.evaluate(x=X_dev,y=y_dev)\n\n","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"f504f1608bcc9966b2e337d6533c7e14792ae93e","_cell_guid":"7455bcc6-dc16-416e-92b0-30050e72da48"},"cell_type":"markdown","source":"### Adam optimizer with regularization\n\nIn our next model, we will stick with the relu activator, but replace the momentum with an Adam optimizer. Adaptive momumtum estimator uses exponentially weighted averages of the gradients to optimize its momentum.  However, since this method is known to overfit the model because of its fast decent, we will make use of a regulizer to avoid overfitting. The l2 regulizer adds the sum of absolute values of the weights to the loss function, thus discouraging large weights that overemphasize single observations."},{"metadata":{"_uuid":"2935031afc6efb8dad4a94b116d7be0a5a9be76c","_cell_guid":"39b86e13-054c-43c5-afad-d3939233997b","trusted":true},"cell_type":"code","source":"# Sequential model\nadam_regularized = Sequential()\n\n# First hidden layer now regularized\nadam_regularized.add(Dense(32,activation='relu',\n                input_dim=X_train.shape[1],\n                kernel_regularizer = regularizers.l2(0.01)))\n\n# Second hidden layer now regularized\nadam_regularized.add(Dense(16,activation='relu',\n                   kernel_regularizer = regularizers.l2(0.01)))\n\n# Output layer stayed sigmoid\nadam_regularized.add(Dense(1,activation='sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adams(lr=learning_rate,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\nadam_regularized.compile(optimizer=adam_optimizer,\n              loss='poisson',\n              metrics=['acc'])\n\n# Train\nhistory_adam_regularized=adam_regularized.fit(X_train, y_train, # Train on training set\n                             epochs=epochs, # We will train over 1,000 epochs\n                             batch_size=batch_s, # Batch size \n                             verbose=0) # Suppress Keras output\nadam_regularized.evaluate(x=X_dev,y=y_dev)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"be485c5ec53353a04ac2de195f388a301fc8c78c","_cell_guid":"a4cdd35c-2263-4b47-89b9-1bfe960bbd8d"},"cell_type":"markdown","source":"### Tanh  optimizer with dropout\n\n"},{"metadata":{"_uuid":"5cb03efae2c0461f6e7ae6d51df8580ef67d5490","_cell_guid":"7864f87b-d7eb-4e37-b020-fe9edaac8864","trusted":true},"cell_type":"code","source":"# Sequential model\ntanh_dropout = Sequential()\n\n# First hidden layer\ntanh_dropout.add(Dense(32,activation='relu',\n                input_dim=X_train.shape[1]))\n\n# Add dropout layer\ntanh_dropout.add(Dropout(rate=0.5))\n\n# Second hidden layer\ntanh_dropout.add(Dense(16,activation='relu'))\n\n\n# Add another dropout layer\ntanh_dropout.add(Dropout(rate=0.5))\n\n# Output layer stayed sigmoid\ntanh_dropout.add(Dense(1,activation='sigmoid'))\n\n# Setup adam optimizer\nadam_optimizer=adams(lr=learning_rate,\n                beta_1=0.9, \n                beta_2=0.999, \n                epsilon=1e-08)\n\n# Compile the model\ntanh_dropout.compile(optimizer=adam_optimizer,\n              loss='cosine_proximity',\n              metrics=['acc'])\n\n# Train\nhistory_dropout = tanh_dropout.fit(X_train, y_train, # Train on training set\n                             epochs=epochs, # We will train over 1,000 epochs\n                             batch_size=batch_s, # Batch size = training set size\n                             verbose=0) # Suppress Keras output\ntanh_dropout.evaluate(x=X_dev,y=y_dev)","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"64b9502fd367d635ffe50502a5e1883b626d99d1","_cell_guid":"440a77e1-89cc-4bca-9bca-53119d8cbabc"},"cell_type":"markdown","source":"### Model evaluation\n \n To evaluate the model, we will look at MAE and accuracy in terms of the number of times it correctly estimated an upward or downward deviation from the median.\n"},{"metadata":{"_uuid":"b2d7c74870cb1ce86d839ec2affb04d149cca8cd","_cell_guid":"5b1cf1a3-8b0d-4578-bde1-98ac940f4d6e","trusted":true},"cell_type":"code","source":"plt.plot(history_lin_reg.history['loss'], label='Logistic regression')\nplt.plot(history_tanh.history['loss'], label='Tanh Model')\nplt.plot(history_relu_momentum.history['loss'], label='Relu Momentum')\n#plt.plot(history_adam_regularized.history['loss'], label='Adam Regularized')\nplt.plot(history_dropout.history['loss'], label='Tanh with Dropout')\nplt.plot(Sequential_feature_scaled.history['loss'], label='New')\n\nplt.xlabel('Epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"33aad3459556e316084142adf760d8d3d7f171ae","_cell_guid":"8be9c22a-3a18-4711-ad91-bad9fd0d100a","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm, cross_validation\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.metrics as skm\nfrom scipy.stats import linregress\nfrom sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\nfrom datetime import datetime\nimport timeit","execution_count":53,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"82a23859e31133c683a29aa3e3f73e9b1e4ee06a","_cell_guid":"2f63edb8-2663-4eee-b650-7530f1ee1fb8","trusted":true},"cell_type":"code","source":"cv_l = cross_validation.KFold(len(X_train), n_folds=10, shuffle=True, random_state = 1)\nregr = LassoCV(cv=cv_l, n_jobs = 2)","execution_count":54,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2ac4ef693daae3535c23b5fe68cffa9ade422562","_cell_guid":"1d5e0fbe-ce95-4e93-90de-d5cd7e8e9da6","trusted":true},"cell_type":"code","source":"regr = regr.fit( X_train, y_train )\ny_pred = regr.predict(X_dev)","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"0daaa6aed614664ff2a093ed7d2e6c01ec667608","_cell_guid":"ff463a7b-de84-45d3-aff4-2b2cd5333d08","trusted":true},"cell_type":"code","source":"from scipy import stats\ny_pred_lin_reg = lin_reg.predict(X_dev,batch_size=batch_s)\ny_pred_tanh_model = tanh_model.predict(X_dev,batch_size=batch_s)\ny_pred_relu_momentum = relu_momentum.predict(X_dev,batch_size=batch_s)\ny_pred_adam_regularized = adam_regularized.predict(X_dev,batch_size=batch_s)\ny_pred_tanh_dropout = tanh_dropout.predict(X_dev,batch_size=batch_s)\ny_pred = model.predict(X_dev,batch_size=batch_s)\nstats.describe(y_pred_lin_reg)","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"a8cc530a7e9a1894c956d4d9a472dba824468734","_cell_guid":"dd75bcf9-0490-4ccf-8bbf-f8191c069773","trusted":true},"cell_type":"code","source":"#http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\ndef plot_prediction(predicted,desciption):\n    fig, ax = plt.subplots()\n    ax.scatter(y_dev, predicted, edgecolors=(0, 0, 0))\n    ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n    ax.set_xlabel('Measured')\n    ax.set_ylabel('Predicted by '+desciption)\n    ax.plot([-30,30], [0,0], 'k-')   \n    ax.plot([0,0], [-30,30], 'k-')\n    plt.show()\n    \nplot_prediction(y_pred, 'new')\nplot_prediction(y_pred_lin_reg, 'linear')\nplot_prediction(y_pred_tanh_model, 'tanh')\nplot_prediction(y_pred_relu_momentum, 'relu momentum')\nplot_prediction(y_pred_adam_regularized, 'adam')\nplot_prediction(y_pred_tanh_dropout,'dropout')\nplot_prediction(y_pred,'lasso regression')","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"33f703c3a164ead12b7d7a872d12d332b2386f19","_cell_guid":"8e82d818-d040-479d-95e0-e9b349dc6f29"},"cell_type":"markdown","source":"Unfortunately, it appears that so far the models do not find any possible improvements over the median sales forecasts with the available explanatory variables."},{"metadata":{"_uuid":"e3c30e44178c49e849d2bc10931a3cce1492b55b","_cell_guid":"70761e61-5956-4731-87a9-29826e2914a6"},"cell_type":"markdown","source":"## Forecasting sales\n\nAfter we have created our model, we can predict things with it on the test set"},{"metadata":{"_uuid":"6bab27bf9920c23be127c60c2373b32c6dfe7bb8","_cell_guid":"1c42f2cd-0a29-4feb-a657-41aefa9ae9bc","trusted":true},"cell_type":"code","source":"test.head()","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"5bd5c5ab3a2a37175273595ce322f74695a984cd","_cell_guid":"410fbefc-a1e3-4370-9efc-959168d9d970","trusted":true},"cell_type":"code","source":"X_test = test.iloc[:,1:(n+1)].values\nX_test.shape\ntest.head()\nfinal_y_prediction = tanh_dropout.predict(X_test,batch_size=batch_s)\n","execution_count":120,"outputs":[]},{"metadata":{"_uuid":"def1162bf32a3f7ae4bb2ac095f563cd3aa2faf7","_cell_guid":"fb84c230-e225-4978-9620-9d3b0d70de8c"},"cell_type":"markdown","source":"To create the ids required for the submission we need the original test file one more time"},{"metadata":{"collapsed":true,"_uuid":"2960d6859c72864b566bf1bbda6303fad93e957c","_cell_guid":"0414f45b-d856-45f8-8d2d-985c660e6e8d","trusted":true},"cell_type":"code","source":"testfile = pd.read_csv('../input/test.csv')","execution_count":124,"outputs":[]},{"metadata":{"_uuid":"04fc28c4aea5110fe9887e3d631407c26b5163d3","_cell_guid":"443f7d4c-a7ab-4df5-953f-db9d4ba391ee"},"cell_type":"markdown","source":"Let's add the means to our testfile and then subtract the expected difference."},{"metadata":{"_uuid":"a897575dc8a9132a63dd27cdb722e1da3e125cac","_cell_guid":"3fdb44d0-07af-40f8-95e9-7d21065a114c","trusted":true},"cell_type":"code","source":"# Create final forecasts\ntestfile['prediction']=final_y_prediction\ntestfile['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in testfile['Date'].astype(str).values.tolist()]\ntestfile['Month'] = [date.month for date in testfile['DateType']]\ntestfile['Month'] = 'Month_' + testfile['Month'].map(str)\n\ntestfile=testfile.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month'])\ntestfile['prediction'].fillna(testfile['prediction'].median(), inplace=True) \ntestfile['Median Sales'].fillna(testfile['Median Sales'].median(), inplace=True) \ntestfile['prediction']+=testfile['Median Sales']\ntestfile.describe()","execution_count":125,"outputs":[]},{"metadata":{"_uuid":"7c8c55cc85f67e658a6ec5a9f1725bef665212ee","_cell_guid":"488e96f0-48ad-4cf6-a824-55cd39d2e45f"},"cell_type":"markdown","source":"Now we create the submission. Once you run the kernel you can download the submission from its outputs and upload it to the Kaggle InClass competition page."},{"metadata":{"_uuid":"f74aaf3d356cbf771e6a1c701831ac96a3b8a4d5","_cell_guid":"982cced0-7d5f-49db-baea-bab0f7c119b8","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'id':testfile['Store'].map(str) + '_' + testfile['Dept'].map(str) + '_' + testfile['Date'].map(str),\n                          'Weekly_Sales':testfile['prediction']})","execution_count":127,"outputs":[]},{"metadata":{"_uuid":"a9c5897cbf36c2686ff8bf35b3042dc28fa31d3c","_cell_guid":"8a336795-931d-4684-80af-c234f413e611"},"cell_type":"markdown","source":"Check submission one more time"},{"metadata":{"_uuid":"edbd0855127394702f79195f064f0eb482ec19cb","_cell_guid":"38946ced-89fd-4e66-afb1-66af80c45086","trusted":true},"cell_type":"code","source":"submission.head()","execution_count":128,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a5549a653482844d45f3812edf87050c08b82cde","_cell_guid":"2138ca94-82cc-4a2f-b754-4a3fb6662541","trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":129,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a60f6757a0b43eed6d2bb250befbe59989b7b8a0","_cell_guid":"84c1aaea-ce6d-440d-84ea-8178aa2c260d","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}